{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied Data Science (MAST30034) Tutorial 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`statsmodels`:\n",
    "- Linear Regression\n",
    "- Evaluation Metrics\n",
    "- Penalized Regression (LASSO and Ridge)\n",
    "\n",
    "`pyspark.ml` (Experimental):\n",
    "- Linear Regression\n",
    "\n",
    "Project 1 Report:\n",
    "- Questions\n",
    "- Ongoing feedback.\n",
    "\n",
    "Optional Content for Students:\n",
    "- Generalised Linear Models (GLM) with `statsmodels`\n",
    "_________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:56:40.074555Z",
     "start_time": "2022-07-18T07:56:40.068541Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from statsmodels.formula.api import ols, glm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:52:56.715236Z",
     "start_time": "2022-07-18T07:52:56.695504Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"../../data/tute_data/sample_data.parquet\")\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, let's try to predict `total_amount` using `fare_amount, tip_amount, toll_amount, trip_distance, VendorID` as predictors.\n",
    "\n",
    "Some things to take note:\n",
    "- `tip_amount` is only valid for `payment_type == 1` (card)\n",
    "- `VendorID` is categorical, with only two possible values (`1` or `2`) so we should make it boolean\n",
    "\n",
    "**Whilst you may use this as an example, please do not copy this as it is incorrect.**\n",
    "\n",
    "How so? Discuss as a class the implications of predicting `total_amount` given the feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:52:56.730594Z",
     "start_time": "2022-07-18T07:52:56.717962Z"
    }
   },
   "outputs": [],
   "source": [
    "# filter dataframe\n",
    "COL_FILTER = ['total_amount', 'fare_amount', 'tip_amount', 'tolls_amount', 'trip_distance', 'VendorID']\n",
    "df_filtered = df.loc[df['payment_type'] == 1, COL_FILTER].reset_index(drop=True)\n",
    "\n",
    "# same as df_filtered['VendorID'].astype(bool)\n",
    "df_filtered['VendorID'] = df_filtered['VendorID'] == 1 \n",
    "\n",
    "df_filtered.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We are looking for linear relationships between our chosen response `total_amount`.   \n",
    "- Now I'm not sure what kind of life you've lived, but I'm fairly certain that we can infer that `total_amount` will have a positive linear relationship with `fare_amount`. Let's see a quick plot..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:52:56.817579Z",
     "start_time": "2022-07-18T07:52:56.731487Z"
    }
   },
   "outputs": [],
   "source": [
    "df_filtered[['total_amount', 'fare_amount']].plot.scatter(x='fare_amount', y='total_amount')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, obviously this looks like an overall positive linear relationship. How might we statistically test this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `R`, we would do something like this for (Ordinary) Least Squares:\n",
    "```R\n",
    ">>> fit <- lm(total_amount~fare_amount + tip_amount + tolls_amount + trip_distance + VendorID ,data=dat_fit)\n",
    ">>> summary(fit)\n",
    "```\n",
    "```\n",
    "Call:\n",
    "lm(formula = total_amount ~ fare_amount + tip_amount + tolls_amount +\n",
    "trip_distance + VendorID, data = dat_fit)\n",
    "\n",
    "Residuals:\n",
    "Min     1Q      Median  3Q     Max\n",
    "-1.4727 -0.3295 -0.1528 0.1747 1.7975\n",
    "\n",
    "Coefficients:\n",
    "               Estimate Std. Error t value Pr(>|t|)\n",
    "(Intercept)    1.162154   0.002986 389.194  <2e-16 ***\n",
    "fare_amount    0.993388   0.000315 3153.943 <2e-16 ***\n",
    "tip_amount     1.006511   0.000826 1218.553 <2e-16 ***\n",
    "tolls_amount   0.979325   0.001285 762.428  <2e-16 ***\n",
    "trip_distance  0.011742   0.000963 12.194   <2e-16 ***\n",
    "VendorIDTRUE  -0.003125   0.002914 -1.073    0.283\n",
    "---\n",
    "Signif. codes:\n",
    "0 ^a˘A¨Y***^a˘A´Z 0.001 ^a˘A¨Y**^a˘A´Z 0.01 ^a˘A¨Y*^a˘A´Z 0.05 ^a˘A¨Y.^a˘A´Z 0.1 ^a˘A¨Y ^a˘A´Z 1\n",
    "\n",
    "Residual standard error: 0.362 on 61886 degrees of freedom\n",
    "Multiple R-squared: 0.9994,          Adjusted R-squared: 0.9994\n",
    "F-statistic: 1.953e+07 on 5 and 61886 DF, p-value: < 2.2e-16\n",
    "```\n",
    "\n",
    "Note: This example from `R` uses an older dataset hence different values to the Python output below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation Source: https://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.OLS.html?highlight=ols#statsmodels.regression.linear_model.OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:52:56.869576Z",
     "start_time": "2022-07-18T07:52:56.818695Z"
    }
   },
   "outputs": [],
   "source": [
    "fit = ols(\n",
    "    formula=\"total_amount ~ fare_amount + tip_amount + tolls_amount + trip_distance + VendorID\",\n",
    "    data=df_filtered\n",
    ").fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:52:56.988049Z",
     "start_time": "2022-07-18T07:52:56.903408Z"
    }
   },
   "outputs": [],
   "source": [
    "print(fit.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion Questions:\n",
    "1. Is this model good? Discuss $R^2$, AIC, and Hypothesis Testing.\n",
    "    \n",
    "2. How might we improve this model? Discuss what we can do with the current features / engineered features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:52:57.073252Z",
     "start_time": "2022-07-18T07:52:56.994715Z"
    }
   },
   "outputs": [],
   "source": [
    "# let's try another model without VendorID\n",
    "fitter = ols(\n",
    "    formula=\"total_amount ~ fare_amount + tip_amount + tolls_amount + trip_distance\",\n",
    "    data=df_filtered\n",
    ").fit()\n",
    "\n",
    "print(fitter.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have to values of AIC to compare with, which one is better...?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:52:57.095241Z",
     "start_time": "2022-07-18T07:52:57.082241Z"
    }
   },
   "outputs": [],
   "source": [
    "fit.aic, fitter.aic, f\"abs diff: {abs(fitter.aic - fit.aic)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Penalized Regression\n",
    "- LASSO (l1) and Ridge (l2) Regression\n",
    "\n",
    "MAST30025 Revision:\n",
    "- Lecture 4 (variable selection)\n",
    "- LSM topic 5 (`ch05_handout`) slide 141/141\n",
    "- An excellent explanation on Ridge / LASSO: https://www.youtube.com/watch?v=9LNpiiKCQUo (recommended at x1.25 speed)\n",
    "\n",
    "Things you might have forgotten when working with penalized models:\n",
    "- Always good to standardize your data prior to train and test. Most models perform poorly if not standardized prior. \n",
    "- Do not fit your standardizer to test, only to train. You should transform both your train and test though.\n",
    "\n",
    "### LASSO ($\\ell_1$)\n",
    "Quick overview:\n",
    "- LASSO may cause coefficients to be set to 0 by constraining the model.\n",
    "- This is because we put a constraint where the sum of the absolute values of the coefficients must be less than some fixed value. \n",
    "- As such, some coefficients may end up having 0 which is the same as *dropping* the attribute from the model. Notably, features that are collinear (correlated) will result in one of them being reduced to 0 coefficient.\n",
    "- In this sense, it's quite similar to feature selection as you end up with a model that is much more simpler. \n",
    "- However, LASSO does not do well when the feature space is small as you may end up with an over-simplified model, as well as cases where all the features are significant or when coefficients are extremely large. \n",
    "- This is why you might want to standardize your dataset prior to fitting.\n",
    "\n",
    "Solution:\n",
    "- Requires an iterative method to solve $(\\mathbf{y}-X\\beta)^T(\\mathbf{y}-X\\beta) + \\lambda I \\beta$\n",
    "- The $\\ell_1$ (vector normal) comes from the penalty term $\\lambda I \\beta$. Our $\\beta$ term is to the power of 1, hence we have $\\ell_1$.\n",
    "\n",
    "### Ridge ($\\ell_2$)\n",
    "Quick overview:\n",
    "- Aims to lower the scale of the coefficients to avoid overfitting, but does not result in coefficients being 0.\n",
    "- In contrast to LASSO, we put a constrain using the sum of squares that must be lest than a fixed value. \n",
    "- As you might guess, this means we still have several features making it less interpretable than LASSO.\n",
    "- However, Ridge Regression performs best in cases where there may be high multi-colinearity (i.e dependencies between attributes) or high linear correlation between certain attributes,\n",
    "- This is because it reduces variance in exchange for some more bias (consider variance-bias tradeoff).\n",
    "- You must also ensure that we have more observations than attributes (`n > p`) as this penalty method does not drop features, leading to worse predictions. \n",
    "\n",
    "Solution:\n",
    "- Closed-form which can be found by minimising $(\\mathbf{y}-X\\beta)^T(\\mathbf{y}-X\\beta) + \\lambda I \\beta^T\\beta$\n",
    "- The $\\ell_2$ (vector normal) comes from the penalty term $\\lambda I \\beta^T\\beta$. Since the $\\beta$ term is squared, we have $\\ell_2$.\n",
    "\n",
    "### Elastic Net\n",
    "Quick overview:\n",
    "- Combines both Ridge and LASSO into a single model utilising an $\\alpha$ parameter, where $\\alpha=0$ is Ridge and $\\alpha=1$ is LASSO.\n",
    "- Implementation Documentation: https://github.com/civisanalytics/python-glmnet/blob/master/glmnet/linear.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:52:57.251463Z",
     "start_time": "2022-07-18T07:52:57.238822Z"
    }
   },
   "outputs": [],
   "source": [
    "yCOLS = ['total_amount']\n",
    "xCOLS = ['fare_amount', 'tip_amount', 'tolls_amount', 'trip_distance', 'VendorID']\n",
    "\n",
    "# standardize (by calculating the zscore) so our data has mean 0 and var 1\n",
    "# alternatively, you can use sklearn's StandardScalar\n",
    "from scipy.stats import zscore\n",
    "\n",
    "df_standard = df_filtered[xCOLS].astype(float).apply(zscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:52:57.290993Z",
     "start_time": "2022-07-18T07:52:57.252558Z"
    }
   },
   "outputs": [],
   "source": [
    "# format output to 4 decimal places\n",
    "pd.options.display.float_format = '{:,.4f}'.format\n",
    "df_standard.describe().loc[['mean','std']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, `df_standard` has  $\\mu=0, \\sigma=1(=\\sigma^2)$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:52:57.522654Z",
     "start_time": "2022-07-18T07:52:57.294040Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from glmnet import ElasticNet\n",
    "\n",
    "elastic_net_model = ElasticNet(alpha=1)\n",
    "elastic_net_model.fit(\n",
    "    df_standard.values, \n",
    "    # use np.squeeze to remove the warning message:\n",
    "    # A column-vector y was passed when a 1d array was expected.\n",
    "    np.squeeze(df_filtered[yCOLS].values)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to look at the shrinking parameter $\\lambda$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:52:57.606100Z",
     "start_time": "2022-07-18T07:52:57.557325Z"
    }
   },
   "outputs": [],
   "source": [
    "# this can be accessed using the .lambda_best_ method after fitting!\n",
    "print(f'Best lambda value for LASSO: {elastic_net_model.lambda_best_[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\lambda$ for `ElasticNet` is computed by using cross validation (iterative approach). \n",
    "\n",
    "What about our coefficients?\n",
    "- https://github.com/civisanalytics/python-glmnet/blob/master/glmnet/linear.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:52:57.645533Z",
     "start_time": "2022-07-18T07:52:57.620201Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    index=['Intercept'] + xCOLS, \n",
    "    data=[elastic_net_model.intercept_] + list(elastic_net_model.coef_), \n",
    "    columns=['Coefficient']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discuss the results.\n",
    "\n",
    "If you want to run predictions with `ElasticNet`, you can use `elastic_net_model.predict(x)` to the predict a new set of observations by passing through the `x` matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression with Spark\n",
    "Whilst using `sklearn` or `statsmodels` can be easier on a smaller sample size, using the full dataset can be quite memory intensive. For those looking to use larger datasets, using the `pyspark.ml` library may prove useful.\n",
    "\n",
    "We'll go back to the first linear model example that we did with `statsmodels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:53:18.089054Z",
     "start_time": "2022-07-18T07:52:57.658185Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MAST30034 Tutorial 3\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:53:23.400992Z",
     "start_time": "2022-07-18T07:53:18.090801Z"
    }
   },
   "outputs": [],
   "source": [
    "sdf = spark.read.parquet('../../data/tlc_data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like correlation from the previous tutorial, we will need to assemble a single vector for `pyspark.ml` to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:53:23.429589Z",
     "start_time": "2022-07-18T07:53:23.402037Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:53:24.449109Z",
     "start_time": "2022-07-18T07:53:23.430527Z"
    }
   },
   "outputs": [],
   "source": [
    "features = \"features\"\n",
    "input_cols = [\"fare_amount\", \"tip_amount\", \"tolls_amount\", \"trip_distance\", \"VendorID\"]\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=input_cols, \n",
    "    outputCol=features\n",
    ")\n",
    "\n",
    "model_sdf = assembler \\\n",
    "            .transform(\n",
    "                sdf.dropna('any')\n",
    "            ) \\\n",
    "            .select('total_amount', features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll start to notice that the PySpark API mirrors `sklearn`, hopefully, this doesn't seem to foreign.\n",
    "\n",
    "Pyspark Docs: https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.regression.LinearRegression.html\n",
    "\n",
    "This implementation supports both OLS, Ridge, LASSO, and Elastic Net. You can change between the models by specifying the `elasticNetParam`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:53:24.452793Z",
     "start_time": "2022-07-18T07:53:24.450397Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:53:44.411600Z",
     "start_time": "2022-07-18T07:53:24.454467Z"
    }
   },
   "outputs": [],
   "source": [
    "lm = LinearRegression(\n",
    "    featuresCol='features', \n",
    "    labelCol='total_amount'\n",
    ").fit(model_sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:53:44.445926Z",
     "start_time": "2022-07-18T07:53:44.412399Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    data=[lm.intercept] + list(lm.coefficients),\n",
    "    index=['intercept'] + input_cols,\n",
    "    columns=['coefficient']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go through a single prediction from the record above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:53:45.511354Z",
     "start_time": "2022-07-18T07:53:44.446643Z"
    }
   },
   "outputs": [],
   "source": [
    "# example record to predict\n",
    "sdf.select('total_amount', *input_cols).limit(1).show(vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:53:46.225675Z",
     "start_time": "2022-07-18T07:53:45.512416Z"
    }
   },
   "outputs": [],
   "source": [
    "# preprocess for predictions\n",
    "predict_test = sdf.select(*input_cols).limit(1)\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=input_cols, \n",
    "    outputCol=features\n",
    ")\n",
    "\n",
    "predict_sdf = assembler.transform(predict_test).select(features)\n",
    "\n",
    "predict_sdf.show(1, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `lm.transform()` to predict an `sdf` containing a single vector of features as shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:53:46.908619Z",
     "start_time": "2022-07-18T07:53:46.226695Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions = lm.transform(predict_sdf)\n",
    "predictions.show(vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at the record above, the true value of `total_amount` is `23.45` with our prediction being `24.42`. Only a single dollar off, not too bad! (or is it?).\n",
    "\n",
    "For evaluation metrics, you can use the `.summary` method. See https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.regression.LinearRegressionModel.html?highlight=ml%20summary%20regression#pyspark.ml.regression.LinearRegressionModel.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:53:46.923654Z",
     "start_time": "2022-07-18T07:53:46.909422Z"
    }
   },
   "outputs": [],
   "source": [
    "# r2 example\n",
    "lm.summary.r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:53:46.928390Z",
     "start_time": "2022-07-18T07:53:46.924481Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# if you want to see the definitive list of all evaluation metrics accessible from lm.summary\n",
    "help(lm.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Notes for Revision\n",
    "#### What is the Bias-Variance tradeoff with respect to linear models:\n",
    "- Less parameters = less variance but more bias\n",
    "- More parameters = more variance but less bias\n",
    "- The goal depends on the problem, but generally we want an even variance and bias (intersection).\n",
    "\n",
    "\n",
    "#### Is using regression on X attribute / specific dataset even a good choice...?\n",
    "- The answer is yes, it is a good choice *to try*\n",
    "- BUT also try other methods...\n",
    "\n",
    "\n",
    "#### What are the pros and cons of stepwise regression?\n",
    "- Forward Selection (start from nothing and end until significant)\n",
    "- Backward Elimination (start with everything and end until no more can be removed)\n",
    "- Not always the best results...\n",
    "\n",
    "\n",
    "#### What is best subset regression and the pros and cons of it?\n",
    "- A brute-force like method of fitting *all possible regressions* or *all possible models*\n",
    "- Unlike stepwise, this method fits all possible models based on the variables specified, so you will get the best model possible\n",
    "![a_secret_easter_egg_i_like_apples](https://i.kym-cdn.com/photos/images/newsfeed/001/718/138/147.jpg)\n",
    "\n",
    "\n",
    "\n",
    "#### What is an assumption we make when we fit linear regression models?\n",
    "- Well, the data has to be linearly separable. \n",
    "- Does this also apply to other models too...? (Recall SVM and kernel function which we can use)\n",
    "- Perhaps another model might suit the dataset... (Trees, Neural Networks, Clustering, etc...)\n",
    "\n",
    "\n",
    "#### If you were to use a decision tree, how would you compare between two different fits? \n",
    "- Look at Gini Impurity (probability of an incorrectly classified instance)\n",
    "\n",
    "\n",
    "#### How about baselines or other predictive machine learning models?\n",
    "- Precision, Recall, Classification Accuracy...\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Fitting a GLM with Python\n",
    "**Since MAST30027 is not a pre-requisuite, we will not cover this nor do we expect students to use GLMs. However, those who wish to experiment with GLMs using Python may go through this section.**\n",
    "\n",
    "Let's go through an example:\n",
    "- The `passenger_count` attribute is discrete and non-negative. If we were to predict it, a linear model will not be sufficient. \n",
    "- We know that a Poisson distribution takes in non-negative integer values, so we can use the Poisson family of GLMs to model this. \n",
    "- We will use `total_amount, trip_distance, VendorID` as our regressors.\n",
    "\n",
    "Summary:\n",
    "- GLM's allow us to express relationships in a linear and additive way like normal linear regression.\n",
    "- However, it might be the case that the underlying true relationship is neither linear nor additive. \n",
    "- The transformation is done through a *link function* (in this case, Poisson).\n",
    "\n",
    "Why would we use try and use Poisson? The distribution of `passenger_count` is frequency based greater than 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T07:56:43.429657Z",
     "start_time": "2022-07-18T07:56:43.206205Z"
    }
   },
   "outputs": [],
   "source": [
    "from statsmodels.api import families\n",
    "\n",
    "# convert VendorID to categorical\n",
    "df['VendorID'] = df['VendorID'] == 1\n",
    "\n",
    "# statsmodels glm\n",
    "fit = glm(\n",
    "    formula=\"passenger_count ~ total_amount + trip_distance + VendorID\",\n",
    "    data=df, \n",
    "    family=families.Poisson()\n",
    ").fit()\n",
    "\n",
    "print(fit.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
